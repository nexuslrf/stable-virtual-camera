{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/ruofanl/Projects/stable-virtual-camera\n"
          ]
        }
      ],
      "source": [
        "cd /home/ruofanl/Projects/stable-virtual-camera"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/home/ruofanl/Projects/video-to-video')\n",
        "sys.path.insert(0, '/home/ruofanl/Projects/diffusers/src')\n",
        "from src.models.custom_unet_mv import UNetMV2DConditionModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "import os.path as osp\n",
        "\n",
        "import fire\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "from seva.data_io import get_parser\n",
        "from seva.eval import (\n",
        "    IS_TORCH_NIGHTLY,\n",
        "    compute_relative_inds,\n",
        "    create_transforms_simple,\n",
        "    infer_prior_inds,\n",
        "    infer_prior_stats,\n",
        "    run_one_scene,\n",
        ")\n",
        "from seva.geometry import (\n",
        "    generate_interpolated_path,\n",
        "    generate_spiral_path,\n",
        "    get_arc_horizontal_w2cs,\n",
        "    get_default_intrinsics,\n",
        "    get_lookat,\n",
        "    get_preset_pose_fov,\n",
        ")\n",
        "from seva.model import SGMWrapper\n",
        "from seva.modules.autoencoder import AutoEncoder\n",
        "from seva.modules.conditioner import CLIPConditioner\n",
        "from seva.sampling import DiscreteDenoiser\n",
        "from seva.utils import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = \"cuda\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL = SGMWrapper(\n",
        "    load_model(\n",
        "        model_version=1.1,\n",
        "        pretrained_model_name_or_path=\"stabilityai/stable-virtual-camera\",\n",
        "        weight_name=\"model.safetensors\",\n",
        "        device=\"cpu\",\n",
        "        verbose=True,\n",
        "    ).eval()\n",
        ") #.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SGMWrapper(\n",
              "  (module): Seva(\n",
              "    (time_embed): Sequential(\n",
              "      (0): Linear(in_features=320, out_features=1280, bias=True)\n",
              "      (1): SiLU()\n",
              "      (2): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "    )\n",
              "    (input_blocks): ModuleList(\n",
              "      (0): TimestepEmbedSequential(\n",
              "        (0): Conv2d(11, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      )\n",
              "      (1-2): 2 x TimestepEmbedSequential(\n",
              "        (0): ResBlock(\n",
              "          (in_layers): Sequential(\n",
              "            (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
              "            (1): SiLU()\n",
              "            (2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (emb_layers): Sequential(\n",
              "            (0): SiLU()\n",
              "            (1): Linear(in_features=1280, out_features=320, bias=True)\n",
              "          )\n",
              "          (dense_emb_layers): Sequential(\n",
              "            (0): Conv2d(6, 640, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (out_layers): Sequential(\n",
              "            (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
              "            (1): SiLU()\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (skip_connection): Identity()\n",
              "        )\n",
              "        (1): MultiviewTransformer(\n",
              "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
              "          (proj_in): Linear(in_features=320, out_features=320, bias=True)\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): TransformerBlock(\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (ff): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
              "                (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "          (proj_out): Linear(in_features=320, out_features=320, bias=True)\n",
              "          (time_mixer): SkipConnect()\n",
              "          (time_mix_blocks): ModuleList(\n",
              "            (0): TransformerBlockTimeMix(\n",
              "              (norm_in): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff_in): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (ff): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
              "                (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): TimestepEmbedSequential(\n",
              "        (0): Downsample(\n",
              "          (op): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (4): TimestepEmbedSequential(\n",
              "        (0): ResBlock(\n",
              "          (in_layers): Sequential(\n",
              "            (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
              "            (1): SiLU()\n",
              "            (2): Conv2d(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (emb_layers): Sequential(\n",
              "            (0): SiLU()\n",
              "            (1): Linear(in_features=1280, out_features=640, bias=True)\n",
              "          )\n",
              "          (dense_emb_layers): Sequential(\n",
              "            (0): Conv2d(6, 640, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (out_layers): Sequential(\n",
              "            (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
              "            (1): SiLU()\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (skip_connection): Conv2d(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): MultiviewTransformer(\n",
              "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "          (proj_in): Linear(in_features=640, out_features=640, bias=True)\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): TransformerBlock(\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (ff): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
              "                (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "          (proj_out): Linear(in_features=640, out_features=640, bias=True)\n",
              "          (time_mixer): SkipConnect()\n",
              "          (time_mix_blocks): ModuleList(\n",
              "            (0): TransformerBlockTimeMix(\n",
              "              (norm_in): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff_in): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (ff): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
              "                (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): TimestepEmbedSequential(\n",
              "        (0): ResBlock(\n",
              "          (in_layers): Sequential(\n",
              "            (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
              "            (1): SiLU()\n",
              "            (2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (emb_layers): Sequential(\n",
              "            (0): SiLU()\n",
              "            (1): Linear(in_features=1280, out_features=640, bias=True)\n",
              "          )\n",
              "          (dense_emb_layers): Sequential(\n",
              "            (0): Conv2d(6, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (out_layers): Sequential(\n",
              "            (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
              "            (1): SiLU()\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (skip_connection): Identity()\n",
              "        )\n",
              "        (1): MultiviewTransformer(\n",
              "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "          (proj_in): Linear(in_features=640, out_features=640, bias=True)\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): TransformerBlock(\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (ff): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
              "                (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "          (proj_out): Linear(in_features=640, out_features=640, bias=True)\n",
              "          (time_mixer): SkipConnect()\n",
              "          (time_mix_blocks): ModuleList(\n",
              "            (0): TransformerBlockTimeMix(\n",
              "              (norm_in): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff_in): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (ff): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
              "                (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (6): TimestepEmbedSequential(\n",
              "        (0): Downsample(\n",
              "          (op): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (7): TimestepEmbedSequential(\n",
              "        (0): ResBlock(\n",
              "          (in_layers): Sequential(\n",
              "            (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
              "            (1): SiLU()\n",
              "            (2): Conv2d(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (emb_layers): Sequential(\n",
              "            (0): SiLU()\n",
              "            (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dense_emb_layers): Sequential(\n",
              "            (0): Conv2d(6, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (out_layers): Sequential(\n",
              "            (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
              "            (1): SiLU()\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (skip_connection): Conv2d(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): MultiviewTransformer(\n",
              "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "          (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): TransformerBlock(\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (ff): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
              "                (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "          (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (time_mixer): SkipConnect()\n",
              "          (time_mix_blocks): ModuleList(\n",
              "            (0): TransformerBlockTimeMix(\n",
              "              (norm_in): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff_in): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (ff): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
              "                (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (8): TimestepEmbedSequential(\n",
              "        (0): ResBlock(\n",
              "          (in_layers): Sequential(\n",
              "            (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
              "            (1): SiLU()\n",
              "            (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (emb_layers): Sequential(\n",
              "            (0): SiLU()\n",
              "            (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dense_emb_layers): Sequential(\n",
              "            (0): Conv2d(6, 2560, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (out_layers): Sequential(\n",
              "            (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
              "            (1): SiLU()\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (skip_connection): Identity()\n",
              "        )\n",
              "        (1): MultiviewTransformer(\n",
              "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "          (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): TransformerBlock(\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (ff): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
              "                (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "          (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (time_mixer): SkipConnect()\n",
              "          (time_mix_blocks): ModuleList(\n",
              "            (0): TransformerBlockTimeMix(\n",
              "              (norm_in): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff_in): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (ff): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
              "                (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (9): TimestepEmbedSequential(\n",
              "        (0): Downsample(\n",
              "          (op): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (10-11): 2 x TimestepEmbedSequential(\n",
              "        (0): ResBlock(\n",
              "          (in_layers): Sequential(\n",
              "            (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
              "            (1): SiLU()\n",
              "            (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (emb_layers): Sequential(\n",
              "            (0): SiLU()\n",
              "            (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dense_emb_layers): Sequential(\n",
              "            (0): Conv2d(6, 2560, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (out_layers): Sequential(\n",
              "            (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
              "            (1): SiLU()\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (skip_connection): Identity()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (middle_block): TimestepEmbedSequential(\n",
              "      (0): ResBlock(\n",
              "        (in_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (emb_layers): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dense_emb_layers): Sequential(\n",
              "          (0): Conv2d(6, 2560, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (out_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (skip_connection): Identity()\n",
              "      )\n",
              "      (1): MultiviewTransformer(\n",
              "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "        (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (transformer_blocks): ModuleList(\n",
              "          (0): TransformerBlock(\n",
              "            (attn1): Attention(\n",
              "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "              (to_out): Sequential(\n",
              "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                (1): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (ff): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): GEGLU(\n",
              "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
              "                )\n",
              "                (1): Dropout(p=0.0, inplace=False)\n",
              "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "              )\n",
              "            )\n",
              "            (attn2): Attention(\n",
              "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "              (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
              "              (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
              "              (to_out): Sequential(\n",
              "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                (1): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        (time_mixer): SkipConnect()\n",
              "        (time_mix_blocks): ModuleList(\n",
              "          (0): TransformerBlockTimeMix(\n",
              "            (norm_in): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "            (ff_in): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): GEGLU(\n",
              "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
              "                )\n",
              "                (1): Dropout(p=0.0, inplace=False)\n",
              "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "              )\n",
              "            )\n",
              "            (attn1): Attention(\n",
              "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "              (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "              (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "              (to_out): Sequential(\n",
              "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                (1): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (ff): FeedForward(\n",
              "              (net): Sequential(\n",
              "                (0): GEGLU(\n",
              "                  (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
              "                )\n",
              "                (1): Dropout(p=0.0, inplace=False)\n",
              "                (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "              )\n",
              "            )\n",
              "            (attn2): Attention(\n",
              "              (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "              (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
              "              (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
              "              (to_out): Sequential(\n",
              "                (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                (1): Dropout(p=0.0, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "        (placeholder): LeakyReLU(negative_slope=0.01)\n",
              "      )\n",
              "      (2): ResBlock(\n",
              "        (in_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (emb_layers): Sequential(\n",
              "          (0): SiLU()\n",
              "          (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (dense_emb_layers): Sequential(\n",
              "          (0): Conv2d(6, 2560, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (out_layers): Sequential(\n",
              "          (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
              "          (1): SiLU()\n",
              "          (2): Dropout(p=0.0, inplace=False)\n",
              "          (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "        (skip_connection): Identity()\n",
              "      )\n",
              "    )\n",
              "    (output_blocks): ModuleList(\n",
              "      (0-1): 2 x TimestepEmbedSequential(\n",
              "        (0): ResBlock(\n",
              "          (in_layers): Sequential(\n",
              "            (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)\n",
              "            (1): SiLU()\n",
              "            (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (emb_layers): Sequential(\n",
              "            (0): SiLU()\n",
              "            (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dense_emb_layers): Sequential(\n",
              "            (0): Conv2d(6, 5120, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (out_layers): Sequential(\n",
              "            (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
              "            (1): SiLU()\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (2): TimestepEmbedSequential(\n",
              "        (0): ResBlock(\n",
              "          (in_layers): Sequential(\n",
              "            (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)\n",
              "            (1): SiLU()\n",
              "            (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (emb_layers): Sequential(\n",
              "            (0): SiLU()\n",
              "            (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dense_emb_layers): Sequential(\n",
              "            (0): Conv2d(6, 5120, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (out_layers): Sequential(\n",
              "            (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
              "            (1): SiLU()\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): Upsample(\n",
              "          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (3-4): 2 x TimestepEmbedSequential(\n",
              "        (0): ResBlock(\n",
              "          (in_layers): Sequential(\n",
              "            (0): GroupNorm32(32, 2560, eps=1e-05, affine=True)\n",
              "            (1): SiLU()\n",
              "            (2): Conv2d(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (emb_layers): Sequential(\n",
              "            (0): SiLU()\n",
              "            (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dense_emb_layers): Sequential(\n",
              "            (0): Conv2d(6, 5120, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (out_layers): Sequential(\n",
              "            (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
              "            (1): SiLU()\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (skip_connection): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): MultiviewTransformer(\n",
              "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "          (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): TransformerBlock(\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (ff): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
              "                (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "          (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (time_mixer): SkipConnect()\n",
              "          (time_mix_blocks): ModuleList(\n",
              "            (0): TransformerBlockTimeMix(\n",
              "              (norm_in): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff_in): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (ff): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
              "                (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "          (placeholder): LeakyReLU(negative_slope=0.01)\n",
              "        )\n",
              "      )\n",
              "      (5): TimestepEmbedSequential(\n",
              "        (0): ResBlock(\n",
              "          (in_layers): Sequential(\n",
              "            (0): GroupNorm32(32, 1920, eps=1e-05, affine=True)\n",
              "            (1): SiLU()\n",
              "            (2): Conv2d(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (emb_layers): Sequential(\n",
              "            (0): SiLU()\n",
              "            (1): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          )\n",
              "          (dense_emb_layers): Sequential(\n",
              "            (0): Conv2d(6, 3840, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (out_layers): Sequential(\n",
              "            (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
              "            (1): SiLU()\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (skip_connection): Conv2d(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): MultiviewTransformer(\n",
              "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
              "          (proj_in): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): TransformerBlock(\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (ff): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
              "                (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "          (proj_out): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "          (time_mixer): SkipConnect()\n",
              "          (time_mix_blocks): ModuleList(\n",
              "            (0): TransformerBlockTimeMix(\n",
              "              (norm_in): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff_in): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_v): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (ff): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): Linear(in_features=1280, out_features=10240, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=1280, out_features=1280, bias=False)\n",
              "                (to_k): Linear(in_features=1024, out_features=1280, bias=False)\n",
              "                (to_v): Linear(in_features=1024, out_features=1280, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "          (placeholder): LeakyReLU(negative_slope=0.01)\n",
              "        )\n",
              "        (2): Upsample(\n",
              "          (conv): Conv2d(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (6): TimestepEmbedSequential(\n",
              "        (0): ResBlock(\n",
              "          (in_layers): Sequential(\n",
              "            (0): GroupNorm32(32, 1920, eps=1e-05, affine=True)\n",
              "            (1): SiLU()\n",
              "            (2): Conv2d(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (emb_layers): Sequential(\n",
              "            (0): SiLU()\n",
              "            (1): Linear(in_features=1280, out_features=640, bias=True)\n",
              "          )\n",
              "          (dense_emb_layers): Sequential(\n",
              "            (0): Conv2d(6, 3840, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (out_layers): Sequential(\n",
              "            (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
              "            (1): SiLU()\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (skip_connection): Conv2d(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): MultiviewTransformer(\n",
              "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "          (proj_in): Linear(in_features=640, out_features=640, bias=True)\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): TransformerBlock(\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (ff): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
              "                (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "          (proj_out): Linear(in_features=640, out_features=640, bias=True)\n",
              "          (time_mixer): SkipConnect()\n",
              "          (time_mix_blocks): ModuleList(\n",
              "            (0): TransformerBlockTimeMix(\n",
              "              (norm_in): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff_in): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (ff): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
              "                (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "          (placeholder): LeakyReLU(negative_slope=0.01)\n",
              "        )\n",
              "      )\n",
              "      (7): TimestepEmbedSequential(\n",
              "        (0): ResBlock(\n",
              "          (in_layers): Sequential(\n",
              "            (0): GroupNorm32(32, 1280, eps=1e-05, affine=True)\n",
              "            (1): SiLU()\n",
              "            (2): Conv2d(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (emb_layers): Sequential(\n",
              "            (0): SiLU()\n",
              "            (1): Linear(in_features=1280, out_features=640, bias=True)\n",
              "          )\n",
              "          (dense_emb_layers): Sequential(\n",
              "            (0): Conv2d(6, 2560, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (out_layers): Sequential(\n",
              "            (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
              "            (1): SiLU()\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (skip_connection): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): MultiviewTransformer(\n",
              "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "          (proj_in): Linear(in_features=640, out_features=640, bias=True)\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): TransformerBlock(\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (ff): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
              "                (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "          (proj_out): Linear(in_features=640, out_features=640, bias=True)\n",
              "          (time_mixer): SkipConnect()\n",
              "          (time_mix_blocks): ModuleList(\n",
              "            (0): TransformerBlockTimeMix(\n",
              "              (norm_in): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff_in): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (ff): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
              "                (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "          (placeholder): LeakyReLU(negative_slope=0.01)\n",
              "        )\n",
              "      )\n",
              "      (8): TimestepEmbedSequential(\n",
              "        (0): ResBlock(\n",
              "          (in_layers): Sequential(\n",
              "            (0): GroupNorm32(32, 960, eps=1e-05, affine=True)\n",
              "            (1): SiLU()\n",
              "            (2): Conv2d(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (emb_layers): Sequential(\n",
              "            (0): SiLU()\n",
              "            (1): Linear(in_features=1280, out_features=640, bias=True)\n",
              "          )\n",
              "          (dense_emb_layers): Sequential(\n",
              "            (0): Conv2d(6, 1920, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (out_layers): Sequential(\n",
              "            (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
              "            (1): SiLU()\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (skip_connection): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): MultiviewTransformer(\n",
              "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
              "          (proj_in): Linear(in_features=640, out_features=640, bias=True)\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): TransformerBlock(\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (ff): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
              "                (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "          (proj_out): Linear(in_features=640, out_features=640, bias=True)\n",
              "          (time_mixer): SkipConnect()\n",
              "          (time_mix_blocks): ModuleList(\n",
              "            (0): TransformerBlockTimeMix(\n",
              "              (norm_in): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff_in): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_v): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (ff): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): Linear(in_features=640, out_features=5120, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): Linear(in_features=2560, out_features=640, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=640, out_features=640, bias=False)\n",
              "                (to_k): Linear(in_features=1024, out_features=640, bias=False)\n",
              "                (to_v): Linear(in_features=1024, out_features=640, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=640, out_features=640, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "          (placeholder): LeakyReLU(negative_slope=0.01)\n",
              "        )\n",
              "        (2): Upsample(\n",
              "          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (9): TimestepEmbedSequential(\n",
              "        (0): ResBlock(\n",
              "          (in_layers): Sequential(\n",
              "            (0): GroupNorm32(32, 960, eps=1e-05, affine=True)\n",
              "            (1): SiLU()\n",
              "            (2): Conv2d(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (emb_layers): Sequential(\n",
              "            (0): SiLU()\n",
              "            (1): Linear(in_features=1280, out_features=320, bias=True)\n",
              "          )\n",
              "          (dense_emb_layers): Sequential(\n",
              "            (0): Conv2d(6, 1920, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (out_layers): Sequential(\n",
              "            (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
              "            (1): SiLU()\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (skip_connection): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): MultiviewTransformer(\n",
              "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
              "          (proj_in): Linear(in_features=320, out_features=320, bias=True)\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): TransformerBlock(\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (ff): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
              "                (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "          (proj_out): Linear(in_features=320, out_features=320, bias=True)\n",
              "          (time_mixer): SkipConnect()\n",
              "          (time_mix_blocks): ModuleList(\n",
              "            (0): TransformerBlockTimeMix(\n",
              "              (norm_in): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff_in): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (ff): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
              "                (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (10-11): 2 x TimestepEmbedSequential(\n",
              "        (0): ResBlock(\n",
              "          (in_layers): Sequential(\n",
              "            (0): GroupNorm32(32, 640, eps=1e-05, affine=True)\n",
              "            (1): SiLU()\n",
              "            (2): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (emb_layers): Sequential(\n",
              "            (0): SiLU()\n",
              "            (1): Linear(in_features=1280, out_features=320, bias=True)\n",
              "          )\n",
              "          (dense_emb_layers): Sequential(\n",
              "            (0): Conv2d(6, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
              "          )\n",
              "          (out_layers): Sequential(\n",
              "            (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
              "            (1): SiLU()\n",
              "            (2): Dropout(p=0.0, inplace=False)\n",
              "            (3): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          )\n",
              "          (skip_connection): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): MultiviewTransformer(\n",
              "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
              "          (proj_in): Linear(in_features=320, out_features=320, bias=True)\n",
              "          (transformer_blocks): ModuleList(\n",
              "            (0): TransformerBlock(\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (ff): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
              "                (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "          (proj_out): Linear(in_features=320, out_features=320, bias=True)\n",
              "          (time_mixer): SkipConnect()\n",
              "          (time_mix_blocks): ModuleList(\n",
              "            (0): TransformerBlockTimeMix(\n",
              "              (norm_in): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (ff_in): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn1): Attention(\n",
              "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_k): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_v): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (ff): FeedForward(\n",
              "                (net): Sequential(\n",
              "                  (0): GEGLU(\n",
              "                    (proj): Linear(in_features=320, out_features=2560, bias=True)\n",
              "                  )\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                  (2): Linear(in_features=1280, out_features=320, bias=True)\n",
              "                )\n",
              "              )\n",
              "              (attn2): Attention(\n",
              "                (to_q): Linear(in_features=320, out_features=320, bias=False)\n",
              "                (to_k): Linear(in_features=1024, out_features=320, bias=False)\n",
              "                (to_v): Linear(in_features=1024, out_features=320, bias=False)\n",
              "                (to_out): Sequential(\n",
              "                  (0): Linear(in_features=320, out_features=320, bias=True)\n",
              "                  (1): Dropout(p=0.0, inplace=False)\n",
              "                )\n",
              "              )\n",
              "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (out): Sequential(\n",
              "      (0): GroupNorm32(32, 320, eps=1e-05, affine=True)\n",
              "      (1): SiLU()\n",
              "      (2): Conv2d(320, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "MODEL.to('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1263968004"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    \"\"\"\n",
        "    Calculate the total number of parameters in a torch.nn.Module.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The model to count parameters for.\n",
        "\n",
        "    Returns:\n",
        "        int: Total number of parameters.\n",
        "    \"\"\"\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "count_parameters(MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "named_params = MODEL.named_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "named_params = list(named_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "keys = [k for k, _ in named_params]\n",
        "np.savetxt('seva_keys.txt', keys, fmt='%s')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unet = UNetMV2DConditionModel.from_pretrained('/home/ruofanl/Projects/exp_outputs/SEVA/unet', subfolder=\"unet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "seva_state_dict = MODEL.state_dict()\n",
        "diff_state_dict = unet.state_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully mapped 1146 keys\n",
            "Conversion complete: 1146 parameters in output state_dict\n"
          ]
        }
      ],
      "source": [
        "from state_dict_converter_simple import convert_seva_to_savediff_state_dict\n",
        "\n",
        "converted_state_dict = convert_seva_to_savediff_state_dict(seva_state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "set()"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "set(converted_state_dict.keys()) - set(diff_state_dict.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check if converted_state_dict and diff_state_dict has the same tensor shape for each k-v\n",
        "for k in converted_state_dict:\n",
        "    if k in diff_state_dict:\n",
        "        if converted_state_dict[k].shape != diff_state_dict[k].shape:\n",
        "            print(f\"Shape mismatch for key '{k}': converted {converted_state_dict[k].shape} vs diff {diff_state_dict[k].shape}\")\n",
        "    else:\n",
        "        print(f\"Key '{k}' not found in diff_state_dict\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unet.load_state_dict(converted_state_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# unet.save_pretrained('/home/ruofanl/Projects/exp_outputs/SEVA/unet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL = SGMWrapper(\n",
        "    load_model(\n",
        "        model_version=1.1,\n",
        "        pretrained_model_name_or_path=\"stabilityai/stable-virtual-camera\",\n",
        "        weight_name=\"model.safetensors\",\n",
        "        device=\"cpu\",\n",
        "        verbose=True,\n",
        "    ).eval()\n",
        ") #.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unet = UNetMV2DConditionModel.from_pretrained('/home/ruofanl/Projects/exp_outputs/SEVA/unet', subfolder=\"unet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = torch.randn(2, 11, 64, 64)\n",
        "t = torch.ones(1)\n",
        "d = torch.randn(2, 6, 64, 64)\n",
        "c = torch.zeros(2, 1, 1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "proj_in: torch.Size([2, 4096, 320]) 3397.57421875\n",
            "block: torch.Size([2, 4096, 320]) 85991.7109375\n",
            "mix_block: torch.Size([2, 4096, 320]) 47366.40234375\n",
            "proj_in: torch.Size([2, 4096, 320]) 16099.732421875\n",
            "block: torch.Size([2, 4096, 320]) 39218.3984375\n",
            "mix_block: torch.Size([2, 4096, 320]) 59618.75\n",
            "proj_in: torch.Size([2, 1024, 640]) 12355.9306640625\n",
            "block: torch.Size([2, 1024, 640]) -24538.669921875\n",
            "mix_block: torch.Size([2, 1024, 640]) -16010.703125\n",
            "proj_in: torch.Size([2, 1024, 640]) -4545.09375\n",
            "block: torch.Size([2, 1024, 640]) 4445.7783203125\n",
            "mix_block: torch.Size([2, 1024, 640]) 10369.7109375\n",
            "proj_in: torch.Size([2, 256, 1280]) 7140.7333984375\n",
            "block: torch.Size([2, 256, 1280]) 7564.3994140625\n",
            "mix_block: torch.Size([2, 256, 1280]) 17577.921875\n",
            "proj_in: torch.Size([2, 256, 1280]) 22444.798828125\n",
            "block: torch.Size([2, 256, 1280]) 4174.41357421875\n",
            "mix_block: torch.Size([2, 256, 1280]) 7135.63525390625\n",
            "Down 0: torch.Size([2, 320, 64, 64]), 13385.8154296875\n",
            "Down 1: torch.Size([2, 320, 64, 64]), -374734.25\n",
            "Down 2: torch.Size([2, 320, 64, 64]), -273368.9375\n",
            "Down 3: torch.Size([2, 320, 32, 32]), 27862.943359375\n",
            "Down 4: torch.Size([2, 640, 32, 32]), -94707.21875\n",
            "Down 5: torch.Size([2, 640, 32, 32]), -17411.732421875\n",
            "Down 6: torch.Size([2, 640, 16, 16]), 22352.96484375\n",
            "Down 7: torch.Size([2, 1280, 16, 16]), -29376.833984375\n",
            "Down 8: torch.Size([2, 1280, 16, 16]), -23917.669921875\n",
            "Down 9: torch.Size([2, 1280, 8, 8]), -19296.2890625\n",
            "Down 10: torch.Size([2, 1280, 8, 8]), -50612.1953125\n",
            "Down 11: torch.Size([2, 1280, 8, 8]), -54988.8203125\n",
            "proj_in: torch.Size([2, 64, 1280]) 1065.8619384765625\n",
            "block: torch.Size([1, 128, 1280]) -31265.30078125\n",
            "mix_block: torch.Size([2, 64, 1280]) 172.9794921875\n",
            "proj_in: torch.Size([2, 256, 1280]) 18042.21484375\n",
            "block: torch.Size([1, 512, 1280]) 50957.26953125\n",
            "mix_block: torch.Size([2, 256, 1280]) -7210.09765625\n",
            "proj_in: torch.Size([2, 256, 1280]) 26492.439453125\n",
            "block: torch.Size([1, 512, 1280]) 68112.9921875\n",
            "mix_block: torch.Size([2, 256, 1280]) -15944.216796875\n",
            "proj_in: torch.Size([2, 256, 1280]) 40852.4296875\n",
            "block: torch.Size([1, 512, 1280]) 23921.056640625\n",
            "mix_block: torch.Size([2, 256, 1280]) -22791.849609375\n",
            "proj_in: torch.Size([2, 1024, 640]) 133082.359375\n",
            "block: torch.Size([1, 2048, 640]) -100214.578125\n",
            "mix_block: torch.Size([2, 1024, 640]) 163530.765625\n",
            "proj_in: torch.Size([2, 1024, 640]) -51900.56640625\n",
            "block: torch.Size([1, 2048, 640]) -161775.0625\n",
            "mix_block: torch.Size([2, 1024, 640]) -14376.673828125\n",
            "proj_in: torch.Size([2, 1024, 640]) 33773.359375\n",
            "block: torch.Size([1, 2048, 640]) 69352.25\n",
            "mix_block: torch.Size([2, 1024, 640]) 8544.1767578125\n",
            "proj_in: torch.Size([2, 4096, 320]) 51561.41796875\n",
            "block: torch.Size([2, 4096, 320]) 37263.203125\n",
            "mix_block: torch.Size([2, 4096, 320]) 212355.609375\n",
            "proj_in: torch.Size([2, 4096, 320]) -4398.03564453125\n",
            "block: torch.Size([2, 4096, 320]) 196477.34375\n",
            "mix_block: torch.Size([2, 4096, 320]) -4778.81005859375\n",
            "proj_in: torch.Size([2, 4096, 320]) -164749.609375\n",
            "block: torch.Size([2, 4096, 320]) -52614.0703125\n",
            "mix_block: torch.Size([2, 4096, 320]) 226.83932495117188\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    y = MODEL.module(x, t, c, d, num_frames=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 4, 64, 64])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "proj_in: torch.Size([2, 4096, 320]) 4796.1162109375\n",
            "block: torch.Size([2, 4096, 320]) 91528.7890625\n",
            "mix_block: torch.Size([2, 4096, 320]) 72354.671875\n",
            "proj_in: torch.Size([2, 4096, 320]) 46760.72265625\n",
            "block: torch.Size([2, 4096, 320]) 24770.029296875\n",
            "mix_block: torch.Size([2, 4096, 320]) 8932.080078125\n",
            "proj_in: torch.Size([2, 1024, 640]) -3866.392333984375\n",
            "block: torch.Size([2, 1024, 640]) -53681.16796875\n",
            "mix_block: torch.Size([2, 1024, 640]) -62955.3984375\n",
            "proj_in: torch.Size([2, 1024, 640]) 1255.724365234375\n",
            "block: torch.Size([2, 1024, 640]) 20222.91796875\n",
            "mix_block: torch.Size([2, 1024, 640]) 28811.916015625\n",
            "proj_in: torch.Size([2, 256, 1280]) 13911.08203125\n",
            "block: torch.Size([2, 256, 1280]) 25709.4375\n",
            "mix_block: torch.Size([2, 256, 1280]) 48988.2265625\n",
            "proj_in: torch.Size([2, 256, 1280]) 3566.4296875\n",
            "block: torch.Size([2, 256, 1280]) -16818.5390625\n",
            "mix_block: torch.Size([2, 256, 1280]) -1035.089599609375\n",
            "  Down block 0: torch.Size([2, 320, 64, 64]) 12496.8515625\n",
            "  Down block 1: torch.Size([2, 320, 64, 64]) -348909.21875\n",
            "  Down block 2: torch.Size([2, 320, 64, 64]) -40593.5703125\n",
            "  Down block 3: torch.Size([2, 320, 32, 32]) 40253.3203125\n",
            "  Down block 4: torch.Size([2, 640, 32, 32]) -25948.7578125\n",
            "  Down block 5: torch.Size([2, 640, 32, 32]) 258303.265625\n",
            "  Down block 6: torch.Size([2, 640, 16, 16]) 51133.1484375\n",
            "  Down block 7: torch.Size([2, 1280, 16, 16]) -10099.9296875\n",
            "  Down block 8: torch.Size([2, 1280, 16, 16]) -78616.9609375\n",
            "  Down block 9: torch.Size([2, 1280, 8, 8]) -41785.83203125\n",
            "  Down block 10: torch.Size([2, 1280, 8, 8]) -72524.75\n",
            "  Down block 11: torch.Size([2, 1280, 8, 8]) -66616.9765625\n",
            "proj_in: torch.Size([2, 64, 1280]) -207.18865966796875\n",
            "block: torch.Size([1, 128, 1280]) 5400.83251953125\n",
            "mix_block: torch.Size([2, 64, 1280]) 15195.267578125\n",
            "proj_in: torch.Size([2, 256, 1280]) 27932.48828125\n",
            "block: torch.Size([1, 512, 1280]) 32018.3828125\n",
            "mix_block: torch.Size([2, 256, 1280]) 29956.162109375\n",
            "proj_in: torch.Size([2, 256, 1280]) 8148.4990234375\n",
            "block: torch.Size([1, 512, 1280]) 10706.71875\n",
            "mix_block: torch.Size([2, 256, 1280]) 11046.3505859375\n",
            "proj_in: torch.Size([2, 256, 1280]) 20123.591796875\n",
            "block: torch.Size([1, 512, 1280]) -20761.232421875\n",
            "mix_block: torch.Size([2, 256, 1280]) -16298.123046875\n",
            "proj_in: torch.Size([2, 1024, 640]) 167802.359375\n",
            "block: torch.Size([1, 2048, 640]) 114129.0234375\n",
            "mix_block: torch.Size([2, 1024, 640]) 142110.828125\n",
            "proj_in: torch.Size([2, 1024, 640]) -12266.25390625\n",
            "block: torch.Size([1, 2048, 640]) -59108.6015625\n",
            "mix_block: torch.Size([2, 1024, 640]) -89212.71875\n",
            "proj_in: torch.Size([2, 1024, 640]) 46169.7109375\n",
            "block: torch.Size([1, 2048, 640]) 37375.26953125\n",
            "mix_block: torch.Size([2, 1024, 640]) 43057.2421875\n",
            "proj_in: torch.Size([2, 4096, 320]) 15213.349609375\n",
            "block: torch.Size([2, 4096, 320]) -46356.46875\n",
            "mix_block: torch.Size([2, 4096, 320]) -11605.4462890625\n",
            "proj_in: torch.Size([2, 4096, 320]) -57602.56640625\n",
            "block: torch.Size([2, 4096, 320]) 142373.46875\n",
            "mix_block: torch.Size([2, 4096, 320]) 164758.125\n",
            "proj_in: torch.Size([2, 4096, 320]) -181800.3125\n",
            "block: torch.Size([2, 4096, 320]) -81110.9140625\n",
            "mix_block: torch.Size([2, 4096, 320]) -113274.9453125\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    yy = unet(x[None, :], t, c, d[None, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 2, 4, 64, 64])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "yy.sample.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 0.0296, -0.2650,  0.2572],\n",
              "         [-0.0848, -0.0635, -0.0595],\n",
              "         [-0.0150,  0.0219,  0.1421]],\n",
              "\n",
              "        [[-0.0394, -0.2449, -0.3784],\n",
              "         [-0.0346,  0.0697,  0.3272],\n",
              "         [-0.5023, -0.1836,  0.1464]],\n",
              "\n",
              "        [[-0.1654, -0.1294, -0.2493],\n",
              "         [ 0.0814,  0.0313, -0.2740],\n",
              "         [-0.2658,  0.3167, -0.1194]],\n",
              "\n",
              "        [[-0.7795,  0.1478, -0.4227],\n",
              "         [-0.1096,  0.1960,  0.5011],\n",
              "         [-0.2916,  0.3913, -0.1746]]])"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y[0, :, :3, :3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'yy' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n",
            "\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m yy\u001b[49m.sample[\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, :, :\u001b[32m3\u001b[39m, :\u001b[32m3\u001b[39m]\n",
            "\n",
            "\u001b[31mNameError\u001b[39m: name 'yy' is not defined"
          ]
        },
        {
          "data": {
            "application/notebook-debug-button": "{\n\t\"notebookUri\": \"file:///home/ruofanl/Projects/stable-virtual-camera/seva_dev.ipynb\"\n}"
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "yy.sample[0, 0, :, :3, :3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Denoising scheduler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "fileHeader": "",
    "fileUid": "dcf74c61-d4b2-4d00-ba18-3114aeaf319b",
    "isAdHoc": false,
    "kernelspec": {
      "display_name": "wai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
